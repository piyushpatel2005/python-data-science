{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> 2c. Loading large datasets progressively with the tf.data.Dataset </h1>\n",
    "\n",
    "In this notebook, we continue reading the same small dataset, but refactor our ML pipeline in two small, but significant, ways:\n",
    "<ol>\n",
    "<li> Refactor the input to read data from disk progressively.\n",
    "<li> Refactor the feature creation so that it is not one-to-one with inputs.\n",
    "</ol>\n",
    "<br/>\n",
    "The Pandas function in the previous notebook first read the whole data into memory -- on a large dataset, this won't be an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> 1. Refactor the input </h2>\n",
    "\n",
    "Read data created in Lab1a, but this time make it more general, so that we can later handle large datasets. We use the Dataset API for this. It ensures that, as data gets delivered to the model in mini-batches, it is loaded from disk only when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
    "DEFAULTS = [[0.0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def decode_csv(row):\n",
    "    columns = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    features.pop('key') # discard, not a real feature\n",
    "    label = features.pop('fare_amount') # remove label from features and store\n",
    "    return features, label\n",
    "\n",
    "  # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "  filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "  # Read lines from text files\n",
    "  textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "  # Parse text lines as comma-separated values (CSV)\n",
    "  dataset = textlines_dataset.map(decode_csv)\n",
    "\n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      num_epochs = None # loop indefinitely\n",
    "      dataset = dataset.shuffle(buffer_size = 10 * batch_size, seed=2)\n",
    "  else:\n",
    "      num_epochs = 1 # end-of-input after this\n",
    "\n",
    "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "def get_train_input_fn():\n",
    "  return read_dataset('./taxi-train.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "  return read_dataset('./taxi-valid.csv', mode = tf.estimator.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> 2. Refactor the way features are created. </h2>\n",
    "\n",
    "For now, pass these through (same as previous lab).  However, refactoring this way will enable us to break the one-to-one relationship between inputs and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('pickuplon'),\n",
    "    tf.feature_column.numeric_column('pickuplat'),\n",
    "    tf.feature_column.numeric_column('dropofflat'),\n",
    "    tf.feature_column.numeric_column('dropofflon'),\n",
    "    tf.feature_column.numeric_column('passengers'),\n",
    "]\n",
    "\n",
    "def add_more_features(feats):\n",
    "  # Nothing to add (yet!)\n",
    "  return feats\n",
    "\n",
    "feature_cols = add_more_features(INPUT_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> Create and train the model </h2>\n",
    "\n",
    "Note that we train for num_steps * batch_size examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0806 02:20:56.595792 140601418356480 estimator.py:1790] Using default config.\n",
      "I0806 02:20:56.598024 140601418356480 estimator.py:209] Using config: {'_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_experimental_distribute': None, '_protocol': None, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_is_chief': True, '_service': None, '_experimental_max_worker_delay_secs': None, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdff000f160>, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_evaluation_master': '', '_task_id': 0, '_tf_random_seed': None, '_eval_distribute': None, '_device_fn': None, '_model_dir': 'taxi_trained', '_master': '', '_global_id_in_cluster': 0, '_train_distribute': None, '_task_type': 'worker'}\n",
      "W0806 02:20:56.622516 140601418356480 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0806 02:20:56.689708 140601418356480 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I0806 02:20:56.707771 140601418356480 estimator.py:1145] Calling model_fn.\n",
      "W0806 02:20:57.087944 140601418356480 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "I0806 02:20:57.374540 140601418356480 estimator.py:1147] Done calling model_fn.\n",
      "I0806 02:20:57.376305 140601418356480 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0806 02:20:57.824130 140601418356480 monitored_session.py:240] Graph was finalized.\n",
      "I0806 02:20:57.924266 140601418356480 session_manager.py:500] Running local_init_op.\n",
      "I0806 02:20:57.937887 140601418356480 session_manager.py:502] Done running local_init_op.\n",
      "I0806 02:20:58.283746 140601418356480 basic_session_run_hooks.py:606] Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "I0806 02:20:59.019744 140601418356480 basic_session_run_hooks.py:262] loss = 100053.76, step = 1\n",
      "I0806 02:21:02.151798 140601418356480 basic_session_run_hooks.py:692] global_step/sec: 31.9247\n",
      "I0806 02:21:02.158949 140601418356480 basic_session_run_hooks.py:260] loss = 37355.76, step = 101 (3.139 sec)\n",
      "I0806 02:21:05.301849 140601418356480 basic_session_run_hooks.py:606] Saving checkpoints for 200 into taxi_trained/model.ckpt.\n",
      "I0806 02:21:05.465340 140601418356480 estimator.py:368] Loss for final step: 46452.137.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressor at 0x7fdff0065f28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "OUTDIR = 'taxi_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "model = tf.estimator.LinearRegressor(\n",
    "      feature_columns = feature_cols, model_dir = OUTDIR)\n",
    "model.train(input_fn = get_train_input_fn, steps = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3> Evaluate model </h3>\n",
    "\n",
    "As before, evaluate on the validation data.  We'll do the third refactoring (to move the evaluation into the training loop) in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 02:21:44.001028 140601418356480 estimator.py:1145] Calling model_fn.\n",
      "I0806 02:21:44.366043 140601418356480 estimator.py:1147] Done calling model_fn.\n",
      "I0806 02:21:44.389069 140601418356480 evaluation.py:255] Starting evaluation at 2019-08-06T02:21:44Z\n",
      "I0806 02:21:44.501119 140601418356480 monitored_session.py:240] Graph was finalized.\n",
      "W0806 02:21:44.503057 140601418356480 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0806 02:21:44.505377 140601418356480 saver.py:1280] Restoring parameters from taxi_trained/model.ckpt-200\n",
      "I0806 02:21:44.568452 140601418356480 session_manager.py:500] Running local_init_op.\n",
      "I0806 02:21:44.600100 140601418356480 session_manager.py:502] Done running local_init_op.\n",
      "I0806 02:21:45.080678 140601418356480 evaluation.py:275] Finished evaluation at 2019-08-06-02:21:45\n",
      "I0806 02:21:45.082067 140601418356480 estimator.py:2039] Saving dict for global step 200: average_loss = 108.82934, global_step = 200, label/mean = 11.666427, loss = 45300.21, prediction/mean = 11.59484\n",
      "I0806 02:21:45.169312 140601418356480 estimator.py:2099] Saving 'checkpoint_path' summary for global step 200: taxi_trained/model.ckpt-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on dataset = 10.432129859924316\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(input_fn = get_valid_input_fn, steps = None)\n",
    "print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "Create a neural network that is capable of finding the volume of a cylinder given the radius of its base (r) and its height (h). Assume that the radius and height of the cylinder are both in the range 0.5 to 2.0. Unlike in the challenge exercise for b_estimator.ipynb, assume that your measurements of r, h and V are all rounded off to the nearest 0.1. Simulate the necessary training dataset. This time, you will need a lot more data to get a good predictor.\n",
    "\n",
    "Hint (highlight to see):\n",
    "<p style='color:white'>\n",
    "Create random values for r and h and compute V. Then, round off r, h and V (i.e., the volume is computed from the true value of r and h; it's only your measurement that is rounded off). Your dataset will consist of the round values of r, h and V. Do this for both the training and evaluation datasets.\n",
    "</p>\n",
    "\n",
    "Now modify the \"noise\" so that instead of just rounding off the value, there is up to a 10% error (uniformly distributed) in the measurement followed by rounding off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
